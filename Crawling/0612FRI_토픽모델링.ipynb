{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토픽 모델링\n",
    "구조화되지 않은 방대한 문헌집단에서 주제를 찾아내기 위한 알고리즘\n",
    "맥락과 관련된 단서들을 이용하여 의미를 가진 단어들을 클러스터링하여 주제를 추론하는 모델\n",
    "감성 분석, 소셜 네트워크 분석 등의 타 분석모델과도 혼합하여 자주 쓰임\n",
    "\n",
    "### LDA(Latent Dirichlet Allocation)\n",
    "* 단어 교환성 가정\n",
    "   * 교환성은 단어들의 순서는 상관하지 않고 오로지 단어들의 유무만이 중요하다는 가정\n",
    "      * 단어들의 순서를 무시할 경우 문서는 단순히 그 안에 포하하는 단어들의 빈도수만을 가지고 표현할 수 있음\n",
    "* 각각의 문서는 여러 개의 주제를 가지고 있음\n",
    "   * [빅데이터, 알고리즘, AI, IoT] -> 데이터 사이언스에 관련된 내용\n",
    "   * [셰익스피어, 톨스토이, 파우스트] -> 문학과 관련된 내용\n",
    "* 자세한 내용은 다음 논문을 참고\n",
    "   * Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research, 3(Jan), 993-1022.\n",
    "* pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# \\w+ 는 알파벳과 숫자가 모두 포함된것이 길이가 상관없이 있는 것\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "stop_words = stopwords.words('english')\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "doc_f = \"Big data is a term used to refer to data sets that are too large or complex for traditional data-processing application software to adequately deal with.\"\n",
    "doc_g = \"Data with many cases offer greater statistical power, while data with higher complexity may lead to a higher false discovery rate.\"\n",
    "doc_h = \"Big data was originally associated with three key concepts: volume, variety, and velocity.\"\n",
    "doc_i = \"A 2016 definition states that 'Big data represents the information assets characterized by such a high volume, velocity and variety to require specific technology and analytical methods for its transformation into value'.\"\n",
    "doc_j = \"Data must be processed with advanced tools to reveal meaningful information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e, doc_f, doc_g, doc_h, doc_i, doc_j]\n",
    "texts = []\n",
    "\n",
    "# 문서별 단어 넣기(소문자화, 토크나이징, 불용어 제거, 어근 추출)\n",
    "for w in doc_set:\n",
    "    raw = w.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in stop_words]\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(85 unique tokens: ['brocolli', 'brother', 'eat', 'good', 'like']...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'brocolli': 0,\n",
       " 'brother': 1,\n",
       " 'eat': 2,\n",
       " 'good': 3,\n",
       " 'like': 4,\n",
       " 'mother': 5,\n",
       " 'around': 6,\n",
       " 'basebal': 7,\n",
       " 'drive': 8,\n",
       " 'lot': 9,\n",
       " 'practic': 10,\n",
       " 'spend': 11,\n",
       " 'time': 12,\n",
       " 'blood': 13,\n",
       " 'caus': 14}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서의 단어들을 사전형(단어토큰:ID) 로 변경\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)\n",
    "\n",
    "# 앞 10개의 단어톸큰 결과 보기\n",
    "dict(list(dictionary.token2id.items())[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)],\n",
       " [(1, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(8, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1)]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서-단어 매트릭스 형성 (bow : bag-of-words)\n",
    "# 문서를 단어토큰의 ID와 빈도수로 수치화\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# (단어 토큰의 ID, 해당 문서에 발생할 빈도수) 앞 3개 문서 결과 보기\n",
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.048*\"data\" + 0.033*\"good\" + 0.032*\"brocolli\" + 0.032*\"eat\" + 0.030*\"brother\"'),\n",
       " (1,\n",
       "  '0.042*\"data\" + 0.036*\"health\" + 0.028*\"big\" + 0.025*\"drive\" + 0.023*\"mother\"'),\n",
       " (2,\n",
       "  '0.044*\"data\" + 0.035*\"higher\" + 0.035*\"may\" + 0.021*\"health\" + 0.020*\"pressur\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA 모형(토픽개수 : 3)\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dicitonary)\n",
    "\n",
    "# 토필별 단어들 출력(토픽별 출력단어들 개수 : 5개)\n",
    "# 단어 옆의 숫자는 가중치(각 토픽에서 해당 단어가 설명하는 비중) 의미\n",
    "ldamodel.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.9309938), (1, 0.03550105), (2, 0.03350517)]\n",
      "[(0, 0.035647858), (1, 0.93039894), (2, 0.03395321)]\n",
      "[(0, 0.031113843), (1, 0.0322805), (2, 0.93660563)]\n",
      "[(0, 0.9453015), (1, 0.027938956), (2, 0.026759543)]\n",
      "[(0, 0.054111574), (1, 0.8967947), (2, 0.049093697)]\n",
      "[(0, 0.95628065), (1, 0.022920791), (2, 0.020798517)]\n",
      "[(0, 0.020214833), (1, 0.020123841), (2, 0.9596613)]\n",
      "[(0, 0.031627245), (1, 0.9365477), (2, 0.031825118)]\n",
      "[(0, 0.018531295), (1, 0.017678464), (2, 0.96379024)]\n",
      "[(0, 0.9239143), (1, 0.038008764), (2, 0.038076896)]\n"
     ]
    }
   ],
   "source": [
    "# 문서별 토픽 분포(i번째 문서에 토픽 0-2 의 분포)\n",
    "# 모든 토픽의 분포 확률의 합은 1\n",
    "for i in range(len(texts)):\n",
    "    print(ldamodel.get_document_topics(corpus)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
