{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전 처리\n",
    "* 말뭉치(corpus)\n",
    "   * 텍스트 마이닝에 적용되는 텍스트 데이터 집합\n",
    "   * 대용량의 <b>정형화된</b> 텍스트 집합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s = \"Hello World\"\n",
    "s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO WORLD'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'서울 부동산 가격이 올해 들어 평균 % 상승했습니다.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(\"[0-9]+\")\n",
    "# substitute \n",
    "p.sub(\"\", \"서울 부동산 가격이 올해 들어 평균 30% 상승했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주제_1 건강한 몸과 건강한 정신 '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \\W(역슬래시 W) : non word 를 표현하며, 알파벳 + 숫자 + _가 아닌 문자를 의미한다.\n",
    "p = re.compile(\"\\W+\")\n",
    "s = p.sub(\" \", \"주제_1: 건강한 몸과 건강한 정신!\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주제 1 건강한 몸과 건강한 정신 '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(\"_\")\n",
    "p.sub(\" \", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 제거¶\n",
    "* 불용어\n",
    "    * 빈번하게 사용되나 구체적인 의미를 찾기 어려운 단어\n",
    "    * NLTK 패키지 이용\n",
    "        * pip install nltk\n",
    "        * 한국어는 지원하지 않음\n",
    "    * 한국어의 경우 분석가가 개인적으로 작성하거나 다른 분석가, 연구자들이 작성한 리스트를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['추석', '연휴', '민족', '대이동', '시작', '교통량', '교통사고', '특히', '자동차', '고장', '상당수', '차지']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_Korean = [\"추석\", \"연휴\", \"민족\", \"대이동\", \"시작\", \"늘어\", \"교통량\", \"교통사고\", \"특히\", \"자동차\", \"고장\", \"상당수\", \"차지\", \"나타\", \"것\", \"기자\"]\n",
    "stopwords = [\"가다\", \"늘어\", \"나타\", \"것\", \"기자\"]\n",
    "\n",
    "[i for i in words_Korean if i not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\330-15\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chief', 'justice', 'roberts', ',', 'president', 'carter', ',', 'president', 'clinton', ',', 'president', 'bush', ',', 'president', 'obama', ',', 'fellow', 'americans', 'people', 'world', ',', 'thank']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "words_English = ['chief', 'justice', 'roberts', ',', 'president', 'carter', ',', 'president', 'clinton', ',', 'president', 'bush', ',', 'president', 'obama', ',', 'fellow', 'americans', 'and', 'people', 'of', 'the', 'world', ',', 'thank', 'you']\n",
    "\n",
    "print([w for w in words_English if not w in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 같은 어근 동일화\n",
    "* 분석 전 어근 동일화(stemming) 과정을 거쳐서 동일한 의미의 단어들을 같은 형태로 통일함\n",
    "* NLTK 패키지의 PorterStemmer 라이브러리 이용\n",
    "    * 이외 LancasterStemmer, RegexpStemmer 라이브러리가 있음\n",
    "* 한국어의 경우 이러한 라이브러리가 없어 형태소 분석기를 사용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\330-15\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is import to be immers while you are python with python . all python have python poorli at least onc . "
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps_stemmer = PorterStemmer()\n",
    "\n",
    "new_text = \"It is important to be immersed while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps_stemmer.stem(w), end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is import to be immers whil you ar python with python . al python hav python poor at least ont . "
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "LS_stemmer = LancasterStemmer()\n",
    "\n",
    "for w in words:\n",
    "    print(LS_stemmer.stem(w), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is important to be immersed while you are ing with  . All ers have ed poorly at least once . "
     ]
    }
   ],
   "source": [
    "from nltk.stem.regexp import RegexpStemmer\n",
    "RS_stemmer = RegexpStemmer(\"python\")\n",
    "\n",
    "for w in words:\n",
    "    print(RS_stemmer.stem(w), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram\n",
    "* n번 연이어 등장하는 단어들의 연쇄를 의미함\n",
    "    * 두 번 연이어 등장하면 바이그램, 세 번 연이어 등장하면 트라이그램으로 부름\n",
    "    * 트라이그램 이상은 보편적으로 활용하지 않음\n",
    "* 보편적으로 영어에만 적용\n",
    "* 바이그램 이상의 엔그램만 독자적으로 활용하는 것은 대단히 위험하며 유니그램(1-gram)과 혼합하여 단어들을 도출하는 것이 가장 이상적임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Chief', 'Justice') ('Justice', 'Roberts,') ('Roberts,', 'President') ('President', 'Carter,') ('Carter,', 'President') ('President', 'Clinton,') ('Clinton,', 'President') ('President', 'Bush,') ('Bush,', 'President') ('President', 'Obama,') ('Obama,', 'fellow') ('fellow', 'Americans') ('Americans', 'and') ('and', 'people') ('people', 'of') ('of', 'the') ('the', 'world,') ('world,', 'thank') ('thank', 'you.') ('you.', 'We,') ('We,', 'the') ('the', 'citizens') ('citizens', 'of') ('of', 'America') ('America', 'are') ('are', 'not') ('not', 'jointed') ('jointed', 'in') ('in', 'a') ('a', 'great') ('great', 'national') ('national', 'effort') ('effort', 'to') ('to', 'rebuild') ('rebuild', 'our') ('our', 'country') ('country', 'and') ('and', 'restore') ('restore', 'its') ('its', 'promise') ('promise', 'for') ('for', 'all') ('all', 'of') ('of', 'our') ('our', 'people.') ('people.', 'Together,') ('Together,', 'we') ('we', 'will') ('will', 'determine') ('determine', 'the') ('the', 'course') ('course', 'of') ('of', 'America') ('America', 'and') ('and', 'the') ('the', 'world') ('world', 'for') ('for', 'many,') ('many,', 'many') ('many', 'years') ('years', 'to') ('to', 'come.') ('come.', 'We') ('We', 'will') ('will', 'face') ('face', 'challenges.') ('challenges.', 'We') ('We', 'will') ('will', 'confront') ('confront', 'hardships,') ('hardships,', 'but') ('but', 'we') ('we', 'will') ('will', 'get') ('get', 'the') ('the', 'job') ('job', 'once.') "
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"Chief Justice Roberts, President Carter, President Clinton, President Bush, President Obama, fellow Americans and people of the world, thank you. We, the citizens of America are not jointed in a great national effort to rebuild our country and restore its promise for all of our people. Together, we will determine the course of America and the world for many, many years to come. We will face challenges. We will confront hardships, but we will get the job once.\"\n",
    "\n",
    "grams = ngrams(sentence.split(), 2)\n",
    "\n",
    "for gram in grams:\n",
    "    print (gram, end=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36] *",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
