{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토픽 모델링\n",
    "구조화되지 않은 방대한 문헌집단에서 주제를 찾아내기 위한 알고리즘\n",
    "맥락과 관련된 단서들을 이용하여 의미를 가진 단어들을 클러스터링하여 주제를 추론하는 모델\n",
    "감성 분석, 소셜 네트워크 분석 등의 타 분석모델과도 혼합하여 자주 쓰임\n",
    "\n",
    "### LDA(Latent Dirichlet Allocation)\n",
    "* 단어 교환성 가정\n",
    "   * 교환성은 단어들의 순서는 상관하지 않고 오로지 단어들의 유무만이 중요하다는 가정\n",
    "      * 단어들의 순서를 무시할 경우 문서는 단순히 그 안에 포하하는 단어들의 빈도수만을 가지고 표현할 수 있음\n",
    "* 각각의 문서는 여러 개의 주제를 가지고 있음\n",
    "   * [빅데이터, 알고리즘, AI, IoT] -> 데이터 사이언스에 관련된 내용\n",
    "   * [셰익스피어, 톨스토이, 파우스트] -> 문학과 관련된 내용\n",
    "* 자세한 내용은 다음 논문을 참고\n",
    "   * Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research, 3(Jan), 993-1022.\n",
    "* pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# \\w+ 는 알파벳과 숫자가 모두 포함된것이 길이가 상관없이 있는 것\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "stop_words = stopwords.words('english')\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "doc_f = \"Big data is a term used to refer to data sets that are too large or complex for traditional data-processing application software to adequately deal with.\"\n",
    "doc_g = \"Data with many cases offer greater statistical power, while data with higher complexity may lead to a higher false discovery rate.\"\n",
    "doc_h = \"Big data was originally associated with three key concepts: volume, variety, and velocity.\"\n",
    "doc_i = \"A 2016 definition states that 'Big data represents the information assets characterized by such a high volume, velocity and variety to require specific technology and analytical methods for its transformation into value'.\"\n",
    "doc_j = \"Data must be processed with advanced tools to reveal meaningful information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e, doc_f, doc_g, doc_h, doc_i, doc_j]\n",
    "texts = []\n",
    "\n",
    "# 문서별 단어 넣기(소문자화, 토크나이징, 불용어 제거, 어근 추출)\n",
    "for w in doc_set:\n",
    "    raw = w.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in stop_words]\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['brocolli', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'brocolli', 'mother'], ['mother', 'spend', 'lot', 'time', 'drive', 'brother', 'around', 'basebal', 'practic'], ['health', 'expert', 'suggest', 'drive', 'may', 'caus', 'increas', 'tension', 'blood', 'pressur'], ['often', 'feel', 'pressur', 'perform', 'well', 'school', 'mother', 'never', 'seem', 'drive', 'brother', 'better'], ['health', 'profession', 'say', 'brocolli', 'good', 'health'], ['big', 'data', 'term', 'use', 'refer', 'data', 'set', 'larg', 'complex', 'tradit', 'data', 'process', 'applic', 'softwar', 'adequ', 'deal'], ['data', 'mani', 'case', 'offer', 'greater', 'statist', 'power', 'data', 'higher', 'complex', 'may', 'lead', 'higher', 'fals', 'discoveri', 'rate'], ['big', 'data', 'origin', 'associ', 'three', 'key', 'concept', 'volum', 'varieti', 'veloc'], ['2016', 'definit', 'state', 'big', 'data', 'repres', 'inform', 'asset', 'character', 'high', 'volum', 'veloc', 'varieti', 'requir', 'specif', 'technolog', 'analyt', 'method', 'transform', 'valu'], ['data', 'must', 'process', 'advanc', 'tool', 'reveal', 'meaning', 'inform']]\n"
     ]
    }
   ],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(85 unique tokens: ['brocolli', 'brother', 'eat', 'good', 'like']...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'brocolli': 0,\n",
       " 'brother': 1,\n",
       " 'eat': 2,\n",
       " 'good': 3,\n",
       " 'like': 4,\n",
       " 'mother': 5,\n",
       " 'around': 6,\n",
       " 'basebal': 7,\n",
       " 'drive': 8,\n",
       " 'lot': 9,\n",
       " 'practic': 10,\n",
       " 'spend': 11,\n",
       " 'time': 12,\n",
       " 'blood': 13,\n",
       " 'caus': 14}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서의 단어들을 사전형(단어토큰:ID) 로 변경\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)\n",
    "\n",
    "# 앞 10개의 단어톸큰 결과 보기\n",
    "dict(list(dictionary.token2id.items())[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)],\n",
       " [(1, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(8, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1)]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서-단어 매트릭스 형성 (bow : bag-of-words)\n",
    "# 문서를 단어토큰의 ID와 빈도수로 수치화\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# (단어 토큰의 ID, 해당 문서에 발생할 빈도수) 앞 3개 문서 결과 보기\n",
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.038*\"data\" + 0.034*\"higher\" + 0.034*\"may\" + 0.027*\"drive\" + 0.026*\"pressur\"'),\n",
       " (1,\n",
       "  '0.035*\"data\" + 0.033*\"good\" + 0.031*\"eat\" + 0.031*\"brocolli\" + 0.030*\"big\"'),\n",
       " (2,\n",
       "  '0.055*\"data\" + 0.026*\"health\" + 0.025*\"inform\" + 0.024*\"process\" + 0.022*\"big\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA 모형(토픽개수 : 3)\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary)\n",
    "\n",
    "# 토필별 단어들 출력(토픽별 출력단어들 개수 : 5개)\n",
    "# 단어 옆의 숫자는 가중치(각 토픽에서 해당 단어가 설명하는 비중) 의미\n",
    "ldamodel.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.037796255), (1, 0.92559445), (2, 0.03660927)]\n",
      "[(0, 0.035321753), (1, 0.03448602), (2, 0.93019223)]\n",
      "[(0, 0.9377111), (1, 0.030620795), (2, 0.031668093)]\n",
      "[(0, 0.94357723), (1, 0.026639657), (2, 0.029783122)]\n",
      "[(0, 0.053151257), (1, 0.05599299), (2, 0.8908558)]\n",
      "[(0, 0.020443086), (1, 0.020927107), (2, 0.95862985)]\n",
      "[(0, 0.9595089), (1, 0.020089692), (2, 0.020401418)]\n",
      "[(0, 0.031805754), (1, 0.93548006), (2, 0.0327142)]\n",
      "[(0, 0.016405549), (1, 0.017335825), (2, 0.96625865)]\n",
      "[(0, 0.037768677), (1, 0.03796281), (2, 0.9242685)]\n"
     ]
    }
   ],
   "source": [
    "# 문서별 토픽 분포(i번째 문서에 토픽 0-2 의 분포)\n",
    "# 모든 토픽의 분포 확률의 합은 1\n",
    "for i in range(len(texts)):\n",
    "    print(ldamodel.get_document_topics(corpus)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "print(\"\\nPerplexity : \", ldamodel.log_perplexity(corpus))\n",
    "# toppn : 상위 N  개의 단어를 이용해서 유사도를 계산하라는 의미\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, \n",
    "                                     texts=texts,\n",
    "                                     dictionary=dictionary,\n",
    "                                     topn=10)\n",
    "coherence_lad = coherence_model_lda.get_coherence()\n",
    "print(\"\\nCoherence Score : \", coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
